{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import sklearn\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from torchtext.datasets import SQuAD1\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/AdmUser/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/AdmUser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "model = gensim.models.Word2Vec(brown.sents())\n",
    "model.save('brown.embedding')\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [],
   "source": [
    "def loadDF(train_iter):\n",
    "    '''\n",
    "\n",
    "    You will use this function to load the dataset into a Pandas Dataframe for processing.\n",
    "\n",
    "    '''\n",
    "    df = {\"question\": [], \"answer\": []}\n",
    "    index = 0\n",
    "    for context, question, answers, indices in train_iter:\n",
    "        if answers[0]:\n",
    "            df[\"question\"].append(question)\n",
    "            df[\"answer\"].append(answers[0])\n",
    "        index += 1\n",
    "    return pd.DataFrame.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = SQuAD1(root='.', split=('train','dev'))\n",
    "train_data = loadDF(train_data)\n",
    "test_data = loadDF(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name,trimMinValue):\n",
    "        self.name = name\n",
    "        self.index = {0:\"<sos>\", 1:\"<eos>\", 2:\"<pad>\", 3:\"<unk>\"}\n",
    "        self.words = {\"<sos>\":0, \"<eos>\":1, \"<pad>\":2, \"<unk>\":3}\n",
    "        self.wordsCounter = {\"<sos>\":0, \"<eos>\":0, \"<pad>\":0, \"<unk>\":0}\n",
    "        self.count = 4\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.trimMinValue = trimMinValue\n",
    "    \n",
    "    def indexWord(self, word):\n",
    "        if word not in self.words:\n",
    "            self.words[word] = self.count\n",
    "            self.wordsCounter[word] = 1\n",
    "            self.index[self.count] = word\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.wordsCounter[word] += 1\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        tokens = self.tokenizeSentence(sentence)\n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            self.indexWord(token)\n",
    "    \n",
    "    def tokenizeSentence(self, sentence): \n",
    "        return self.tokenizer.tokenize(sentence)\n",
    "    \n",
    "    def trimVocab(self):\n",
    "        trimmedIndex = {0:\"<sos>\", 1:\"<eos>\", 2:\"<pad>\", 3:\"<unk>\"}\n",
    "        trimmedWords = {\"<sos>\":0, \"<eos>\":1, \"<pad>\":2, \"<unk>\":3}\n",
    "        trimmedWordsCounter = {\"<sos>\":0, \"<eos>\":0, \"<pad>\":0, \"<unk>\":0}\n",
    "        trimmedCount = 4\n",
    "        for i in range(4,self.count):\n",
    "            if self.wordsCounter[self.index[i]] >= self.trimMinValue:\n",
    "                trimmedWords[self.index[i]] = trimmedCount\n",
    "                trimmedWordsCounter[self.index[i]] = self.wordsCounter[self.index[i]]\n",
    "                trimmedIndex[trimmedCount] = self.index[i]\n",
    "                trimmedCount += 1\n",
    "        self.index = trimmedIndex\n",
    "        self.words = trimmedWords\n",
    "        self.wordsCounter = trimmedWordsCounter\n",
    "        self.count = trimmedCount\n",
    "    \n",
    "        \n",
    "    def prepareSentence(self, sentence):\n",
    "        tokens = self.tokenizeSentence(sentence)\n",
    "        sentence = []\n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            if token in self.words:\n",
    "                sentence.append(token)\n",
    "            else:\n",
    "                sentence.append(\"<unk>\")\n",
    "        sentence.insert(0, \"<sos>\")\n",
    "        sentence.append(\"<eos>\")\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def paddSentence(self, sentence, maxlen):\n",
    "        paddedSentence = []\n",
    "        if len(sentence)>maxlen:\n",
    "            for i in range(maxlen-1):\n",
    "                paddedSentence.append(sentence[i])\n",
    "            paddedSentence.append(\"<eos>\")\n",
    "        elif len(sentence)<=maxlen:\n",
    "            paddedSentence = sentence\n",
    "            for i in range(len(sentence),maxlen):\n",
    "                paddedSentence.append(\"<pad>\")\n",
    "        return paddedSentence\n",
    "    \n",
    "    def paddSentences(self, sentences, maxlen):\n",
    "        paddedSentences = []\n",
    "        for sentence in sentences:\n",
    "            paddedSentences.append(self.paddSentence(sentence, maxlen))\n",
    "        return paddedSentences\n",
    "        \n",
    "    def indexSentence(self,sentence):\n",
    "        return [self.words[w] for w in sentence]\n",
    "    \n",
    "    def wordSentence(self,sentence):\n",
    "        return [self.index[w] for w in sentence]\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(name='SQuAD1Vocab', trimMinValue = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 55995 words to our vocabulary\n"
     ]
    }
   ],
   "source": [
    "#Add train and test data to vocab\n",
    "\n",
    "for i,r, in train_data.iterrows():\n",
    "  question_text = vocab.addSentence(r[\"question\"])\n",
    "  answer_text = vocab.addSentence(r[\"answer\"])\n",
    "for i,r, in test_data.iterrows():\n",
    "  question_text = vocab.addSentence(r[\"question\"])\n",
    "  answer_text = vocab.addSentence(r[\"answer\"])\n",
    "print(\"Added {} words to our vocabulary\".format(vocab.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain 9959 words in our vocabulary after trimming\n"
     ]
    }
   ],
   "source": [
    "#Trim vocab\n",
    "\n",
    "vocab.trimVocab()\n",
    "print(\"Remain {} words in our vocabulary after trimming\".format(vocab.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare train and test sentences\n",
    "trainQ = []\n",
    "trainA = []\n",
    "testQ = []\n",
    "testA = []\n",
    "\n",
    "for i,r, in train_data.iterrows():\n",
    "    trainQ.append(vocab.prepareSentence(r[\"question\"]))\n",
    "    trainA.append(vocab.prepareSentence(r[\"answer\"]))\n",
    "for i,r, in test_data.iterrows():\n",
    "    testQ.append(vocab.prepareSentence(r[\"question\"]))\n",
    "    testA.append(vocab.prepareSentence(r[\"answer\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'what', 'is', 'the', '<unk>', 'at', 'notre', 'dame', '<eos>']\n",
      "['<sos>', 'what', 'is', 'the', '<unk>', 'at', 'notre', 'dame', '<eos>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(trainQ[3])\n",
    "#Padd sentences\n",
    "maxlen = 10\n",
    "trainQ = vocab.paddSentences(trainQ, maxlen)\n",
    "trainA = vocab.paddSentences(trainA, maxlen)\n",
    "testQ = vocab.paddSentences(testQ, maxlen)\n",
    "testA = vocab.paddSentences(testA, maxlen)\n",
    "print(trainQ[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 5, 6, 7, 8, 9, 10, 11, 1]\n"
     ]
    }
   ],
   "source": [
    "#Tokenize sentences\n",
    "trainQtok = []\n",
    "trainAtok = []\n",
    "testQtok = []\n",
    "testAtok = []\n",
    "\n",
    "for sentence in trainQ:\n",
    "    trainQtok.append(vocab.indexSentence(sentence))\n",
    "for sentence in trainA:\n",
    "    trainAtok.append(vocab.indexSentence(sentence))\n",
    "for sentence in testQ:\n",
    "    testQtok.append(vocab.indexSentence(sentence))\n",
    "for sentence in testA:\n",
    "    testAtok.append(vocab.indexSentence(sentence))\n",
    "\n",
    "print(trainQtok[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(questions, answers, batch_size):\n",
    "\n",
    "    n_batches = len(questions)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    questions = questions[:n_batches*batch_size]\n",
    "    answers = answers[:n_batches*batch_size]\n",
    "\n",
    "    for idx in range(0, len(questions), batch_size):\n",
    "        questions_batch, answers_batch = [], []\n",
    "        questions_batch = np.array(questions[idx:idx+batch_size])\n",
    "        answers_batch = np.array(answers[idx:idx+batch_size])\n",
    "        yield questions_batch, answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers, p):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout = p, batch_first = True)\n",
    "\n",
    "    def forward(self, i):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src vector\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        embedding = self.dropout(self.embedding(i))\n",
    "\n",
    "        o, (h, c) = self.lstm(embedding)\n",
    "        \n",
    "        return h, c\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, embedding_size, output_size, num_layers, p):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout = p, batch_first = True)\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)    \n",
    "        \n",
    "    def forward(self, i, h, c):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''\n",
    "        i = i.unsqueeze(-1)\n",
    "        embedding = self.dropout(self.embedding(i))\n",
    "\n",
    "        o, (h, c) = self.lstm(embedding, (h, c))\n",
    "\n",
    "        pred = self.fc(o)\n",
    "\n",
    "        pred = pred.squeeze(1)\n",
    "\n",
    "        return pred, h, c\n",
    "        \n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):      \n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "\n",
    "        prediction = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        h, c = self.encoder(src)\n",
    "\n",
    "        #sos token -> trg[batch_size, seq_len]\n",
    "        i = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            o, h, c = self.decoder(i, h, c)\n",
    "            prediction[:, t] = o\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = o.argmax(1) \n",
    "            i = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (embedding): Embedding(9959, 256)\n",
      "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (embedding): Embedding(9959, 256)\n",
      "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
      "    (fc): Linear(in_features=512, out_features=9959, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 256\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = vocab.count\n",
    "output_size = vocab.count\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "p_dropout = 0.3\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "enc = Encoder(input_size, hidden_size, embedding_size, num_layers, p_dropout)\n",
    "dec = Decoder(hidden_size, embedding_size, output_size, num_layers, p_dropout)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = vocab.words['<pad>'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", verbose=True, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    l = 0\n",
    "    for x, y in get_batches(trainQtok, trainAtok, batch_size):\n",
    "        \n",
    "        inputs = torch.from_numpy(x).to(device)\n",
    "        targets = torch.from_numpy(y).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(inputs, targets, teacher_forcing_ratio)\n",
    "\n",
    "        #output [batch size, trg len, output dim]->[batch size * trg len, output dim]\n",
    "        #targets [batch size, trg len, output dim]->[batch size * trg len]\n",
    "        #ignore first token\n",
    "        loss = criterion(output[1:].view(-1, output.shape[-1]), targets[1:].view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        l += 1\n",
    "        \n",
    "    return epoch_loss / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        l = 0\n",
    "        for x, y in get_batches(testQtok, testAtok, batch_size):\n",
    "\n",
    "            inputs = torch.from_numpy(x).to(device)\n",
    "            targets = torch.from_numpy(y).to(device)\n",
    "\n",
    "            output = model(inputs, targets, teacher_forcing_ratio = 0)\n",
    "\n",
    "            loss = criterion(output[1:].view(-1, output.shape[-1]), targets[1:].view(-1))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            l += 1\n",
    "        \n",
    "    return epoch_loss / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "writer = SummaryWriter(log_dir=\"./runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | TrainLoss: 7.503294228113186 | BestTrainLoss: 7.503294228113186\n",
      "Epoch: 1 | TrainLoss: 7.317041139156498 | BestTrainLoss: 7.317041139156498\n",
      "Epoch: 2 | TrainLoss: 6.7162404659895865 | BestTrainLoss: 6.7162404659895865\n",
      "Epoch: 3 | TrainLoss: 6.709279183058711 | BestTrainLoss: 6.709279183058711\n",
      "Epoch: 4 | TrainLoss: 6.715953029387179 | BestTrainLoss: 6.709279183058711\n",
      "Epoch: 5 | TrainLoss: 6.623720691915144 | BestTrainLoss: 6.623720691915144\n",
      "Epoch: 6 | TrainLoss: 6.517546980004561 | BestTrainLoss: 6.517546980004561\n",
      "Epoch: 7 | TrainLoss: 6.4598141525223935 | BestTrainLoss: 6.4598141525223935\n",
      "Epoch: 8 | TrainLoss: 6.456882379208392 | BestTrainLoss: 6.456882379208392\n",
      "Epoch: 9 | TrainLoss: 6.528585609636809 | BestTrainLoss: 6.456882379208392\n",
      "Epoch: 10 | TrainLoss: 6.54226643579048 | BestTrainLoss: 6.456882379208392\n",
      "Epoch: 11 | TrainLoss: 6.487820318567823 | BestTrainLoss: 6.456882379208392\n",
      "Epoch: 12 | TrainLoss: 6.4762889335030005 | BestTrainLoss: 6.456882379208392\n",
      "Epoch: 13 | TrainLoss: 6.542937750007674 | BestTrainLoss: 6.456882379208392\n",
      "Epoch: 14 | TrainLoss: 6.495829980972915 | BestTrainLoss: 6.456882379208392\n",
      "Epoch: 15 | TrainLoss: 6.473538586967869 | BestTrainLoss: 6.456882379208392\n",
      "Epoch: 16 | TrainLoss: 6.506523093284919 | BestTrainLoss: 6.456882379208392\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: 17 | TrainLoss: 6.501749286874693 | BestTrainLoss: 6.456882379208392\n",
      "Epoch: 18 | TrainLoss: 6.116381844581916 | BestTrainLoss: 6.116381844581916\n",
      "Epoch: 19 | TrainLoss: 6.032218099337572 | BestTrainLoss: 6.032218099337572\n",
      "Epoch: 20 | TrainLoss: 6.020882981562475 | BestTrainLoss: 6.020882981562475\n",
      "Epoch: 21 | TrainLoss: 6.016424156768977 | BestTrainLoss: 6.016424156768977\n",
      "Epoch: 22 | TrainLoss: 6.008464804866858 | BestTrainLoss: 6.008464804866858\n",
      "Epoch: 23 | TrainLoss: 6.008162113658169 | BestTrainLoss: 6.008162113658169\n",
      "Epoch: 24 | TrainLoss: 6.003721654066566 | BestTrainLoss: 6.003721654066566\n",
      "Epoch: 25 | TrainLoss: 6.002015642255371 | BestTrainLoss: 6.002015642255371\n",
      "Epoch: 26 | TrainLoss: 5.998356593282599 | BestTrainLoss: 5.998356593282599\n",
      "Epoch: 27 | TrainLoss: 5.99394340821874 | BestTrainLoss: 5.99394340821874\n",
      "Epoch: 28 | TrainLoss: 5.988717134933026 | BestTrainLoss: 5.988717134933026\n",
      "Epoch: 29 | TrainLoss: 5.986347230554324 | BestTrainLoss: 5.986347230554324\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 30 | TrainLoss: 5.984946858813191 | BestTrainLoss: 5.984946858813191\n",
      "Epoch: 31 | TrainLoss: 5.9359419527109605 | BestTrainLoss: 5.9359419527109605\n",
      "Epoch: 32 | TrainLoss: 5.933310440409254 | BestTrainLoss: 5.933310440409254\n",
      "Epoch: 33 | TrainLoss: 5.933951637201142 | BestTrainLoss: 5.933310440409254\n",
      "Epoch: 34 | TrainLoss: 5.931509439011066 | BestTrainLoss: 5.931509439011066\n",
      "Epoch: 35 | TrainLoss: 5.926728715673525 | BestTrainLoss: 5.926728715673525\n",
      "Epoch: 36 | TrainLoss: 5.9145489631340515 | BestTrainLoss: 5.9145489631340515\n",
      "Epoch: 37 | TrainLoss: 5.907283050972119 | BestTrainLoss: 5.907283050972119\n",
      "Epoch: 38 | TrainLoss: 5.901285158960443 | BestTrainLoss: 5.901285158960443\n",
      "Epoch: 39 | TrainLoss: 5.90938387279622 | BestTrainLoss: 5.901285158960443\n",
      "Epoch: 40 | TrainLoss: 5.897160679276227 | BestTrainLoss: 5.897160679276227\n",
      "Epoch: 41 | TrainLoss: 5.910033730735556 | BestTrainLoss: 5.897160679276227\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 42 | TrainLoss: 5.907156785329183 | BestTrainLoss: 5.897160679276227\n",
      "Epoch: 43 | TrainLoss: 5.9003665600603785 | BestTrainLoss: 5.897160679276227\n",
      "Epoch: 44 | TrainLoss: 5.900883641159325 | BestTrainLoss: 5.897160679276227\n",
      "Epoch: 45 | TrainLoss: 5.89016034031472 | BestTrainLoss: 5.89016034031472\n",
      "Epoch: 46 | TrainLoss: 5.898723842107762 | BestTrainLoss: 5.89016034031472\n",
      "Epoch: 47 | TrainLoss: 5.897571884400663 | BestTrainLoss: 5.89016034031472\n",
      "Epoch: 48 | TrainLoss: 5.887515579747875 | BestTrainLoss: 5.887515579747875\n",
      "Epoch: 49 | TrainLoss: 5.895212527604131 | BestTrainLoss: 5.887515579747875\n"
     ]
    }
   ],
   "source": [
    "best_train_loss = float('inf')\n",
    "clip = 1\n",
    "save_path = 'chatbot_model.pt'\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, optimizer, criterion, clip)\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print(\"Epoch: \" + str(epoch) + \" | TrainLoss: \" + str(train_loss) + \" | BestTrainLoss: \" + str(best_train_loss))\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    \n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestLoss: 6.404298619526188\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('chatbot_model.pt'))\n",
    "test_loss = evaluate(model, criterion)\n",
    "print(\"TestLoss: \" + str(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
